<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Guto@Site</title>
    <link href="https://gutocarvalho.net/feed.xml" rel="self" />
    <link href="https://gutocarvalho.net" />
    <updated>2021-11-02T11:37:55-03:00</updated>
    <author>
        <name>Guto Carvalho</name>
    </author>
    <id>https://gutocarvalho.net</id>

    <entry>
        <title>DROPS: Criando um Cluster K8S com EKSCTL</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/criando-um-cluster-k8s-com-eksctl/"/>
        <id>https://gutocarvalho.net/criando-um-cluster-k8s-com-eksctl/</id>
            <category term="K8S"/>
            <category term="Drops"/>

        <updated>2021-11-02T11:37:55-03:00</updated>
            <summary>
                <![CDATA[
                    Aprende aí como fazer isso com uma mão nas costas! Criando um&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprende aí como fazer isso com uma mão nas costas!</p>
<hr>
<p>Criando um cluster para testes de forma rápida e rasteira, vamo nessa!</p>
<pre><code>$ eksctl create cluster \
    --region us-east-1 \
    --name falagutera \
    --version 1.21 \
    --node-type t3.xlarge \
    --nodes 3 \
    --ssh-public-key &quot;~/nativetrail.pub&quot;
</code></pre>
<p>gerando o kubeconfig</p>
<pre><code>$ aws eks --region us-east-1 update-kubeconfig --name falagutera
$ export KUBECONFIG=config
</code></pre>
<p>checando o cluster baby</p>
<pre><code>$ kubectl get nodes
</code></pre>
<p>terminou o teste? bora derrubar tudo!</p>
<pre><code>$ eksctl delete cluster falagutera
</code></pre>
<p>pronto!</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>DROPS: Usando EFS em um Cluster K8S EC2</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/usando-efs-em-um-cluster-k8s-ec2/"/>
        <id>https://gutocarvalho.net/usando-efs-em-um-cluster-k8s-ec2/</id>
            <category term="K8S"/>
            <category term="Drops"/>

        <updated>2021-10-25T08:03:34-03:00</updated>
            <summary>
                <![CDATA[
                    Aprenda a usar o EFS Provisioner em um Cluster K8S EC2/RKE. São&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprenda a usar o EFS Provisioner em um Cluster K8S EC2/RKE.</p>
<hr>
<h3 id="o-que-são-drops">O que são drops?</h3>
<p>São DUMPs mentais rápidos e rasteiros, simples e objetivos – que funcionam. </p>
<p>Geralmente de algo que eu acabei de fazer.</p>
<p>Eu – quase sempre – volto para detalhar mais cada passo.</p>
<p>Considere com a mesma qualidade de um rascunho ou uma anotação rápida.</p>
<p>De qualquer forma comenta ai qquer coisa, os comentários estão ligados nos DROPS ;)</p>
<h3 id="veio-a-demanda">Veio a Demanda!</h3>
<p>A ideia é instalar um EFS Provisioner no cluster para poder subir APPS que montam o mesmo volume em diferentes PODs.</p>
<h3 id="então-comofaz">Então ComoFaz?</h3>
<p>Crie o arquivo instala_efs_provisioner.yaml</p>
<pre><code>vim aplica.yaml
</code></pre>
<p>Insira o conteúdo abaixo</p>
<pre><code>---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: efs-provisioner-runner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-efs-provisioner
subjects:
  - kind: ServiceAccount
    name: efs-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: efs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-efs-provisioner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;endpoints&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-efs-provisioner
subjects:
  - kind: ServiceAccount
    name: efs-provisioner
    namespace: kube-system
roleRef:
  kind: Role
  name: leader-locking-efs-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: efs-provisioner
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: efs-provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: efs-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: efs-provisioner
    spec:
      serviceAccount: efs-provisioner
      containers:
        - name: efs-provisioner
          image: quay.io/external_storage/efs-provisioner:latest
          env:
            - name: FILE_SYSTEM_ID
              valueFrom:
                configMapKeyRef:
                  name: efs-provisioner
                  key: file.system.id
            - name: AWS_REGION
              valueFrom:
                configMapKeyRef:
                  name: efs-provisioner
                  key: aws.region
            - name: DNS_NAME
              valueFrom:
                configMapKeyRef:
                  name: efs-provisioner
                  key: dns.name
                  optional: true
            - name: PROVISIONER_NAME
              valueFrom:
                configMapKeyRef:
                  name: efs-provisioner
                  key: provisioner.name
          volumeMounts:
            - name: pv-volume
              mountPath: /persistentvolumes
      volumes:
        - name: pv-volume
          nfs:
            server: fs-&lt;ID DO SEU EFS&gt;.efs.&lt;REGIAO AWS&gt;.amazonaws.com
            path: /
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: aws-efs
provisioner: nativetrail.io/aws-efs
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: efs-provisioner
data:
  file.system.id: fs-&lt;ID DO SEU EFS&gt;
  aws.region: &lt;REGIAO AWS&gt;
  provisioner.name: nativetrail.io/aws-efs
  dns.name: &quot;&quot;
</code></pre>
<p>Ajuste o ID do seu EFS e região da AWS no manifesto acima, e prestenção, tem que o usar o namespace <strong>kube-system</strong> pois ele é especificado nas <strong>roles</strong> e <strong>serviceaccount</strong>, se instalar o deployment em outro namespace não vai funcionar.</p>
<pre><code>kubectl apply -f instala_efs_provisioner.yaml -n kube-system
    
</code></pre>
<p>Pronto, vamos verificar</p>
<pre><code>kubectl get sc
</code></pre>
<p>Saída esperada</p>
<pre><code>NAME                PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
aws-efs             nativetrail.io/aws-efs   Delete          Immediate           false                  43m
aws-ebs (default)   ebs.csi.aws.com          Delete          Immediate           false                  3d16h
</code></pre>
<p>agora vamos testar, crie o manifesto valida-pod-efs.ym com o conteúdo abaixo</p>
<pre><code>---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: valida-aws-efs
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: aws-efs
  resources:
    requests:
      storage: 1Gi
</code></pre>
<p>aplique</p>
<pre><code>kubectl apply -f pod-efs.yaml
</code></pre>
<p>saída</p>
<pre><code>persistentvolumeclaim/valida-aws-efs created
</code></pre>
<p>vamos ver</p>
<pre><code>kubectl get pvc valida-aws-efs
</code></pre>
<p>saída</p>
<pre><code>NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
valida-aws-efs   Bound    pvc-c524234f-4b07-4248-b328-c6c164dddbc2   1Gi        RWX            aws-efs        3m42s
</code></pre>
<p>agora com mais detalhes</p>
<pre><code>kubectl describe pvc valida-aws-efs
</code></pre>
<p>saída</p>
<pre><code>Name:          valida-aws-efs
Namespace:     default
StorageClass:  aws-efs
Status:        Bound
Volume:        pvc-c524234f-4b07-4248-b328-c6c164dddbc2
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-provisioner: nativetrail.io/aws-efs
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      1Gi
Access Modes:  RWX
VolumeMode:    Filesystem
Used By:       &lt;none&gt;
Events:
  Type    Reason                 Age                    From                                                                                   Message
  ----    ------                 ----                   ----                                                                                   -------
  Normal  Provisioning           3m49s                  nativetrail.io/aws-efs_efs-provisioner-644f9f8c8c-n6n48_be97e87c-7e5b-4b42-8756-2adb23a5a105  External provisioner is provisioning volume for claim &quot;default/valida-aws-efs&quot;
  Normal  ProvisioningSucceeded  3m49s                  nativetrail.io/aws-efs_efs-provisioner-644f9f8c8c-n6n48_be97e87c-7e5b-4b42-8756-2adb23a5a105  Successfully provisioned volume pvc-c524234f-4b07-4248-b328-c6c164dddbc2
</code></pre>
<p>Perfeito, tudo funcionando, agora é só usar!</p>
<p>:)</p>
<h3 id="refs">refs</h3>
<ul>
<li><a href="https://github.com/rancher/rke">https://github.com/rancher/rke</a></li>
<li><a href="https://kubernetes.io/pt-br/">https://kubernetes.io/pt-br/</a></li>
<li><a href="https://aws.amazon.com/efs/">https://aws.amazon.com/efs/</a></li>
<li><a href="https://github.com/kubernetes-retired/external-storage/tree/master/aws/efs">https://github.com/kubernetes-retired/external-storage/tree/master/aws/efs</a></li>
<li><a href="https://icicimov.github.io/blog/virtualization/Kubernetes-NFS-shared-storage-in-AWS-with-EFS/">https://icicimov.github.io/blog/virtualization/Kubernetes-NFS-shared-storage-in-AWS-with-EFS/</a></li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>DROPS: Centralizando Logs do K8S com o Loki</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/centralizando-logs-do-k8s-com-o-loki/"/>
        <id>https://gutocarvalho.net/centralizando-logs-do-k8s-com-o-loki/</id>
            <category term="Projetos"/>
            <category term="Drops"/>

        <updated>2021-10-22T09:47:50-03:00</updated>
            <summary>
                <![CDATA[
                    Aprenda a instalar o LOKI da GrafanaLabs para centralizar Logs do K8S.
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprenda a instalar o LOKI da GrafanaLabs para centralizar Logs do K8S.</p>
<hr>
<h3 id="o-que-são-drops">O que são drops?</h3>
<p>São DUMPs mentais rápidos e rasteiros, simples e objetivos – que funcionam. </p>
<p>Geralmente de algo que eu acabei de fazer.</p>
<p>Eu – quase sempre – volto para detalhar mais cada passo.</p>
<p>Considere com a mesma qualidade de um rascunho ou uma anotação rápida.</p>
<p>De qualquer forma comenta ai qquer coisa, os comentários estão ligados nos DROPS ;)</p>
<h3 id="demanda">Demanda!</h3>
<p>A ideia é instalar um Loki no cluster K8S e começar a centralizar e consumir os logs via dashboard do grafana.</p>
<h3 id="por-que-o-loki">Por que o Loki?</h3>
<p>O Loki é leve – em resumo esse é o maior e melhor motivo para usar ele.</p>
<p>Comparando com outras STACKS, ele nao leva 1/3 ou 2/3 do seu cluster só para centralizar logs.</p>
<p>Você acompanha os logs pelo Grafana Dashboard, simples e direto.</p>
<p>Além disso o projeto tem 14 mil estrelas no GitHub e mais de 400 contribuidores :)</p>
<p>Conheça mais do projeto, dá um chance vai :)</p>
<ul>
<li><a href="https://grafana.com/docs/loki/latest/overview/">https://grafana.com/docs/loki/latest/overview/</a></li>
<li><a href="https://grafana.com/docs/loki/latest/overview/comparisons/">https://grafana.com/docs/loki/latest/overview/comparisons/</a></li>
</ul>
<h3 id="antes-de-começar">Antes de começar</h3>
<p>Estou partindo do pressuposto de que você não tem o grafana instalado!</p>
<p>Precisa ter o Helm instalado e um cluster k8s funcional.</p>
<h3 id="então-comofaz">Então ComoFaz?</h3>
<p>adicione o repositorio e atualize os índices</p>
<pre><code>helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
</code></pre>
<p>instale o loki</p>
<pre><code>helm upgrade --install loki --namespace=loki grafana/loki alertmanager.persistentVolume.enabled=true
</code></pre>
<p>instale o grafana</p>
<pre><code>helm install loki-grafana grafana/grafana
</code></pre>
<p>pegue a senha de admin do grafana</p>
<pre><code>kubectl get secret --namespace &lt;YOUR-NAMESPACE&gt; loki-grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode ; echo
</code></pre>
<p>crie um port-forward para acessar o grafana</p>
<pre><code>kubectl port-forward --namespace &lt;YOUR-NAMESPACE&gt; service/loki-grafana 3000:80
</code></pre>
<p>acesse o grafana</p>
<pre><code>http://localhost:3000
</code></pre>
<p>adicione o datasource conforme a doc abaixo</p>
<ul>
<li><a href="https://grafana.com/docs/loki/latest/getting-started/grafana/">https://grafana.com/docs/loki/latest/getting-started/grafana/</a></li>
</ul>
<p>Pronto, divirta-se!</p>
<h3 id="refs">Refs</h3>
<ul>
<li><a href="https://grafana.com/docs/loki/latest/installation/helm/">https://grafana.com/docs/loki/latest/installation/helm/</a></li>
<li><a href="https://grafana.com/go/webinar/loki-getting-started-emea">https://grafana.com/go/webinar/loki-getting-started-emea</a></li>
<li><a href="https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki">https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki</a></li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>DROPS: Instalando Rancher 2.6 em HA</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/drops-instalando-rancher-26-em-ha/"/>
        <id>https://gutocarvalho.net/drops-instalando-rancher-26-em-ha/</id>
            <category term="Projetos"/>
            <category term="Drops"/>

        <updated>2021-10-22T09:14:26-03:00</updated>
            <summary>
                <![CDATA[
                    Aprenda a instalar o novíssimo Rancher 2.6 em modo HA. São DUMPs&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprenda a instalar o novíssimo Rancher 2.6 em modo HA.</p>
<hr>
<h3 id="o-que-são-drops">O que são drops?</h3>
<p>São DUMPs mentais rápidos e rasteiros, simples e objetivos – que funcionam. </p>
<p>Geralmente de algo que eu acabei de fazer.</p>
<p>Eu – quase sempre – volto para detalhar mais cada passo.</p>
<p>Considere com a mesma qualidade de um rascunho ou uma anotação rápida.</p>
<p>De qualquer forma comenta ai qquer coisa, os comentários estão ligados nos DROPS ;)</p>
<h2 id="demanda">Demanda!</h2>
<p>A ideia é instalar um Rancher em HA, cluster de 3 nodes, usando instâncias AWS EC2.</p>
<h2 id="comofaz">ComoFaz?</h2>
<h3 id="antes-de-começar">antes de começar</h3>
<p>Confere aí :)</p>
<ol>
<li>Tenha o Kubectl instalado</li>
<li>Tenha o RKE instalado</li>
<li>Tenha o Helm instalado</li>
<li>Tenha Conta na AWS</li>
<li>Internet é de bom tom :)</li>
</ol>
<h3 id="instâncias-ec2">instâncias ec2</h3>
<p>Vamos lá!</p>
<ol>
<li>crie um chave ssh no ec2</li>
<li>crie 3 instâncias EC2 em sua conta AWS com essa chave ssh</li>
<li>crie 2 target groups rancher-80 e rancher-443 apontando para as máquinas do cluster</li>
<li>crie o load balancer NLB com dois listeners apontando para os target groups rancher-80 e rancher-443</li>
<li>crie uma entrada de dns <strong>rancher.seudominio.tld</strong> apontando para o CNAME do Load Balancer</li>
<li>crie um security group para liberar acesso a porta 80 e 443 as máquinas do cluster</li>
<li>crie um security group para que voce possa instalar o cluster a partir do seu IP via rke (all ports).</li>
</ol>
<h3 id="preparando-instâncias">preparando instâncias</h3>
<p>instale docker nas 3 maquinas</p>
<pre><code>curl https://releases.rancher.com/install-docker/20.10.sh | sh
</code></pre>
<p>habilite e inicie</p>
<pre><code>systemctl enable docker &amp;&amp; systemctl start docker
</code></pre>
<p>coloque o ubuntu no grupo docker</p>
<pre><code>gpasswd -a ubuntu docker
</code></pre>
<h3 id="preparando-e-instalando-o-cluster-da-sua-máquina">preparando e instalando o cluster (da sua máquina)</h3>
<p>criando configuracao</p>
<pre><code>rke config
</code></pre>
<p>ele vai te fazer umas perguntinhas, cadastre apenas 1 node para facilitar, eu usei 1.1.1.1 como exemplo, aponte o local da sua chave ssh, a mesma que usou nos EC2.</p>
<pre><code>[+] Cluster Level SSH Private Key Path [~/.ssh/id_rsa]:
[+] Number of Hosts [1]:
[+] SSH Address of host (1) [none]: 1.1.1.1
[+] SSH Port of host (1) [22]:
[+] SSH Private Key Path of host (1.1.1.1) [none]:
[-] You have entered empty SSH key path, trying fetch from SSH key parameter
[+] SSH Private Key of host (1.1.1.1) [none]:
[-] You have entered empty SSH key, defaulting to cluster level SSH key: ~/.ssh/id_rsa
[+] SSH User of host (1.1.1.1) [ubuntu]:
[+] Is host (1.1.1.1) a Control Plane host (y/n)? [y]: y
[+] Is host (1.1.1.1) a Worker host (y/n)? [n]: y
[+] Is host (1.1.1.1) an etcd host (y/n)? [n]: y
[+] Override Hostname of host (1.1.1.1) [none]:
[+] Internal IP of host (1.1.1.1) [none]:
[+] Docker socket path on host (1.1.1.1) [/var/run/docker.sock]:
[+] Network Plugin Type (flannel, calico, weave, canal, aci) [canal]:
[+] Authentication Strategy [x509]:
[+] Authorization Mode (rbac, none) [rbac]:
[+] Kubernetes Docker image [rancher/hyperkube:v1.21.5-rancher1]:
[+] Cluster domain [cluster.local]:
[+] Service Cluster IP Range [10.43.0.0/16]:
[+] Enable PodSecurityPolicy [n]:
[+] Cluster Network CIDR [10.42.0.0/16]:
[+] Cluster DNS Service IP [10.43.0.10]:
[+] Add addon manifest URLs or YAML files [no]:
</code></pre>
<p>abra o arquivo e coloque os demais nodos, não se esqueça de cadastrar o ip privado também, depois que finalizar com os nodos, personalize o que for necessário para seu ambiente k8s, o rke é beeem flexível quanto a isso, além de ser o instalador de k8s mais fácil que eu conheço.</p>
<pre><code>nodes:
- address: 1.1.1.1
  port: &quot;22&quot;
  internal_address: &quot;172.31.1.1&quot;
  role:
  - controlplane
  - worker
  - etcd
  hostname_override: &quot;rancher_a&quot;
  user: ubuntu
  docker_socket: /var/run/docker.sock
  ssh_key: &quot;&quot;
  ssh_key_path: ~/.ssh/id_rsa
  ssh_cert: &quot;&quot;
  ssh_cert_path: &quot;&quot;
  labels: {}
  taints: []
- address: 2.2.2.2
  port: &quot;22&quot;
  internal_address: &quot;172.31.1.2&quot;
  role:
  - controlplane
  - worker
  - etcd
  hostname_override: &quot;rancher_b&quot;
  user: ubuntu
  docker_socket: /var/run/docker.sock
  ssh_key: &quot;&quot;
  ssh_key_path: ~/.ssh/id_rsa
  ssh_cert: &quot;&quot;
  ssh_cert_path: &quot;&quot;
  labels: {}
  taints: []
- address: 3.3.3.3
  port: &quot;22&quot;
  internal_address: &quot;172.31.1.3&quot;
  role:
  - controlplane
  - worker
  - etcd
  hostname_override: &quot;rancher_c&quot;
  user: ubuntu
  docker_socket: /var/run/docker.sock
  ssh_key: &quot;&quot;
  ssh_key_path: ~/.ssh/id_rsa
  ssh_cert: &quot;&quot;
  ssh_cert_path: &quot;&quot;
  labels: {}
  taints: []
services:
  etcd:
    image: &quot;&quot;
    extra_args: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_binds: []
    win_extra_env: []
    external_urls: []
    ca_cert: &quot;&quot;
    cert: &quot;&quot;
    key: &quot;&quot;
    path: &quot;&quot;
    uid: 0
    gid: 0
    snapshot: null
    retention: &quot;&quot;
    creation: &quot;&quot;
    backup_config: null
  kube-api:
    image: &quot;&quot;
    extra_args: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_binds: []
    win_extra_env: []
    service_cluster_ip_range: 10.43.0.0/16
    service_node_port_range: &quot;&quot;
    pod_security_policy: false
    always_pull_images: false
    secrets_encryption_config: null
    audit_log: null
    admission_configuration: null
    event_rate_limit: null
  kube-controller:
    image: &quot;&quot;
    extra_args: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_binds: []
    win_extra_env: []
    cluster_cidr: 10.42.0.0/16
    service_cluster_ip_range: 10.43.0.0/16
  scheduler:
    image: &quot;&quot;
    extra_args: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_binds: []
    win_extra_env: []
  kubelet:
    image: &quot;&quot;
    extra_args: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_binds: []
    win_extra_env: []
    cluster_domain: cluster.local
    infra_container_image: &quot;&quot;
    cluster_dns_server: 10.43.0.10
    fail_swap_on: false
    generate_serving_certificate: false
  kubeproxy:
    image: &quot;&quot;
    extra_args: {}
    extra_binds: []
    extra_env: []
    win_extra_args: {}
    win_extra_binds: []
    win_extra_env: []
network:
  plugin: canal
  options: {}
  mtu: 0
  node_selector: {}
  update_strategy: null
  tolerations: []
authentication:
  strategy: x509
  sans: []
  webhook: null
addons: &quot;&quot;
addons_include: []
system_images:
  etcd: rancher/mirrored-coreos-etcd:v3.4.16-rancher1
  alpine: rancher/rke-tools:v0.1.78
  nginx_proxy: rancher/rke-tools:v0.1.78
  cert_downloader: rancher/rke-tools:v0.1.78
  kubernetes_services_sidecar: rancher/rke-tools:v0.1.78
  kubedns: rancher/mirrored-k8s-dns-kube-dns:1.17.4
  dnsmasq: rancher/mirrored-k8s-dns-dnsmasq-nanny:1.17.4
  kubedns_sidecar: rancher/mirrored-k8s-dns-sidecar:1.17.4
  kubedns_autoscaler: rancher/mirrored-cluster-proportional-autoscaler:1.8.3
  coredns: rancher/mirrored-coredns-coredns:1.8.4
  coredns_autoscaler: rancher/mirrored-cluster-proportional-autoscaler:1.8.3
  nodelocal: rancher/mirrored-k8s-dns-node-cache:1.18.0
  kubernetes: rancher/hyperkube:v1.21.5-rancher1
  flannel: rancher/mirrored-coreos-flannel:v0.14.0
  flannel_cni: rancher/flannel-cni:v0.3.0-rancher6
  calico_node: rancher/mirrored-calico-node:v3.19.2
  calico_cni: rancher/mirrored-calico-cni:v3.19.2
  calico_controllers: rancher/mirrored-calico-kube-controllers:v3.19.2
  calico_ctl: rancher/mirrored-calico-ctl:v3.19.2
  calico_flexvol: rancher/mirrored-calico-pod2daemon-flexvol:v3.19.2
  canal_node: rancher/mirrored-calico-node:v3.19.2
  canal_cni: rancher/mirrored-calico-cni:v3.19.2
  canal_controllers: rancher/mirrored-calico-kube-controllers:v3.19.2
  canal_flannel: rancher/mirrored-coreos-flannel:v0.14.0
  canal_flexvol: rancher/mirrored-calico-pod2daemon-flexvol:v3.19.2
  weave_node: weaveworks/weave-kube:2.8.1
  weave_cni: weaveworks/weave-npc:2.8.1
  pod_infra_container: rancher/mirrored-pause:3.4.1
  ingress: rancher/nginx-ingress-controller:nginx-0.48.1-rancher1
  ingress_backend: rancher/mirrored-nginx-ingress-controller-defaultbackend:1.5-rancher1
  ingress_webhook: rancher/mirrored-jettech-kube-webhook-certgen:v1.5.1
  metrics_server: rancher/mirrored-metrics-server:v0.5.0
  windows_pod_infra_container: rancher/kubelet-pause:v0.1.6
  aci_cni_deploy_container: noiro/cnideploy:5.1.1.0.1ae238a
  aci_host_container: noiro/aci-containers-host:5.1.1.0.1ae238a
  aci_opflex_container: noiro/opflex:5.1.1.0.1ae238a
  aci_mcast_container: noiro/opflex:5.1.1.0.1ae238a
  aci_ovs_container: noiro/openvswitch:5.1.1.0.1ae238a
  aci_controller_container: noiro/aci-containers-controller:5.1.1.0.1ae238a
  aci_gbp_server_container: noiro/gbp-server:5.1.1.0.1ae238a
  aci_opflex_server_container: noiro/opflex-server:5.1.1.0.1ae238a
ssh_key_path: ~/.ssh/id_rsa
ssh_cert_path: &quot;&quot;
ssh_agent_auth: false
authorization:
  mode: rbac
  options: {}
ignore_docker_version: null
enable_cri_dockerd: null
kubernetes_version: &quot;&quot;
private_registries: []
ingress:
  provider: &quot;&quot;
  options: {}
  node_selector: {}
  extra_args: {}
  dns_policy: &quot;&quot;
  extra_envs: []
  extra_volumes: []
  extra_volume_mounts: []
  update_strategy: null
  http_port: 0
  https_port: 0
  network_mode: &quot;&quot;
  tolerations: []
  default_backend: null
  default_http_backend_priority_class_name: &quot;&quot;
  nginx_ingress_controller_priority_class_name: &quot;&quot;
cluster_name: &quot;&quot;
cloud_provider:
  name: &quot;&quot;
prefix_path: &quot;&quot;
win_prefix_path: &quot;&quot;
addon_job_timeout: 0
bastion_host:
  address: &quot;&quot;
  port: &quot;&quot;
  user: &quot;&quot;
  ssh_key: &quot;&quot;
  ssh_key_path: &quot;&quot;
  ssh_cert: &quot;&quot;
  ssh_cert_path: &quot;&quot;
  ignore_proxy_env_vars: false
monitoring:
  provider: &quot;&quot;
  options: {}
  node_selector: {}
  update_strategy: null
  replicas: null
  tolerations: []
  metrics_server_priority_class_name: &quot;&quot;
restore:
  restore: false
  snapshot_name: &quot;&quot;
rotate_encryption_key: false
dns: null
</code></pre>
<p>depois de cadastrar os nodos, vamos começar o provisionamento</p>
<pre><code>rke up —config cluster.yaml
</code></pre>
<p>após instalar verá a seguinte mensagem ( se tudo der certo )</p>
<pre><code>INFO[0255] Finished building Kubernetes cluster successfully
</code></pre>
<p>configure o kubectl</p>
<pre><code>export KUBECONFIG=kube_config_cluster.yml
</code></pre>
<p>verifique se o cluster está funcionando</p>
<pre><code>kubctl get nodes
</code></pre>
<p>saída exemplo</p>
<pre><code>NAME             STATUS   ROLES                      AGE   VERSION
54.225.184.xxx   Ready    controlplane,etcd,worker   12m   v1.21.5
54.243.65.xxx    Ready    controlplane,etcd,worker   12m   v1.21.5
72.44.32.xxx     Ready    controlplane,etcd,worker   12m   v1.21.5
</code></pre>
<p>cluster instalado, agora verifique a saúde de seu cluster</p>
<pre><code>kubectl get pods --all-namespaces
</code></pre>
<p>saída exemplo</p>
<pre><code>NAMESPACE       NAME                                       READY   STATUS      RESTARTS   AGE
ingress-nginx   default-http-backend-6977475d9b-z64cj      1/1     Running     0          17m
ingress-nginx   nginx-ingress-controller-h4c67             1/1     Running     0          17m
ingress-nginx   nginx-ingress-controller-kjd8r             1/1     Running     0          17m
ingress-nginx   nginx-ingress-controller-q4lp2             1/1     Running     0          17m
kube-system     calico-kube-controllers-7d5d95c8c9-mcfj6   1/1     Running     0          18m
kube-system     canal-lsbkv                                2/2     Running     0          18m
kube-system     canal-lwtq9                                2/2     Running     0          18m
kube-system     canal-wm7c8                                2/2     Running     0          18m
kube-system     coredns-55b58f978-jnfsq                    1/1     Running     0          17m
kube-system     coredns-55b58f978-wr4w7                    1/1     Running     0          18m
kube-system     coredns-autoscaler-76f8869cc9-lz44p        1/1     Running     0          18m
kube-system     metrics-server-55fdd84cd4-rjk7w            1/1     Running     0          18m
kube-system     rke-coredns-addon-deploy-job-th472         0/1     Completed   0          18m
kube-system     rke-ingress-controller-deploy-job-59cm4    0/1     Completed   0          17m
kube-system     rke-metrics-addon-deploy-job-94b8t         0/1     Completed   0          18m
kube-system     rke-network-plugin-deploy-job-q5nts        0/1     Completed   0          18m
</code></pre>
<p>aparentemente tudo bem :)</p>
<h3 id="preparando-e-instalando-o-cert-manager-neste-cluster-sua-máquina">preparando e instalando o cert-manager neste cluster (sua máquina)</h3>
<p>instale os crds do cert-manager</p>
<pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml
</code></pre>
<p>adicione o repo e atualize o índices</p>
<pre><code>helm repo add jetstack https://charts.jetstack.io. 
helm repo update
</code></pre>
<p>crie o namespace</p>
<pre><code>kubectl create namespace cert-manager
</code></pre>
<p>instale o cert-mamanger</p>
<pre><code>helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.5.1
</code></pre>
<p>crie o cluster issuer, sem isso ele não gera os certs via lets</p>
<pre><code>apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
  namespace: cert-manager
spec:
  acme:
    # The ACME server URL
    server: https://acme-v02.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: certmanager@nativetrail.io
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-prod
    # Enable the HTTP-01 challenge provider
    solvers:
    - http01:
        ingress:
          class: nginx
</code></pre>
<p>aplique o issuer</p>
<pre><code>kubectl apply -f issuer.yml
</code></pre>
<p>saída esperada</p>
<pre><code> clusterissuer.cert-manager.io/letsencrypt-prod created
</code></pre>
<p>verifique se está ok</p>
<pre><code>kubectl get clusterissuer 
</code></pre>
<p>saída esperada</p>
<pre><code>NAME               READY   AGE
letsencrypt-prod   True    48s
</code></pre>
<p>se estiver mostrando “True” deu certo!</p>
<h3 id="preparando-e-instalando-o-rancher-neste-cluster-na-sua-máquina">preparando e instalando o rancher neste cluster (na sua máquina)</h3>
<p>adicione o repo e atualize os índices</p>
<pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
helm repo update
</code></pre>
<p>crie o namespace cattle-system</p>
<pre><code>kubectl create namespace cattle-system
</code></pre>
<p>instale o rancher</p>
<pre><code>helm install rancher rancher-latest/rancher \
  --namespace cattle-system \
  --set hostname=kloud.gr1d.io \
  --set replicas=3 \
  --set ingress.tls.source=letsEncrypt \
  --set letsEncrypt.email=certmanager@nativetrail.io
</code></pre>
<p>verifique</p>
<pre><code>kubectl get pods -n cattle-system
</code></pre>
<p>saída esperada, estará criando na primeira vez que rodar o comando</p>
<pre><code>NAME                       READY   STATUS              RESTARTS   AGE
rancher-58b56d54df-7mv7d   0/1     ContainerCreating   0          29s
rancher-58b56d54df-csmpl   0/1     ContainerCreating   0          29s
rancher-58b56d54df-pkpz6   0/1     ContainerCreating   0          29s
</code></pre>
<p>aguarde e verifique novamente, use o mesmo comando</p>
<pre><code>kubectl get pods -n cattle-system
</code></pre>
<p>saída</p>
<pre><code>NAME                       READY   STATUS    RESTARTS   AGE
rancher-58b56d54df-7mv7d   0/1     Running   0          41s
rancher-58b56d54df-csmpl   0/1     Running   0          41s
rancher-58b56d54df-pkpz6   0/1     Running   0          41s
</code></pre>
<p>rancher instalado e rodando!</p>
<h3 id="pós-instalação">pós instalação</h3>
<p>observe a saída do comando helm, ele vai te dizer como pegar a senha gerada para o primeiro acesso, depois disso, acesse o rancher via web através da URL definida e siga as os procedimentos para trocar a senha e iniciar o uso do seu rancher.</p>
<h3 id="refs">refs</h3>
<ul>
<li><a href="https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/">https://rancher.com/docs/rancher/v2.6/en/installation/install-rancher-on-k8s/</a></li>
<li><a href="https://rancher.com/docs/rancher/v2.6/en/installation/resources/k8s-tutorials/infrastructure-tutorials/infra-for-ha/">https://rancher.com/docs/rancher/v2.6/en/installation/resources/k8s-tutorials/infrastructure-tutorials/infra-for-ha/</a></li>
</ul>
<p>:)</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Ferramentas APM open source Self-Hosted</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/ferramentas-open-source-apm/"/>
        <id>https://gutocarvalho.net/ferramentas-open-source-apm/</id>
            <category term="Projetos"/>

        <updated>2021-10-17T16:34:29-03:00</updated>
            <summary>
                <![CDATA[
                    Conheça algumas ferramentas open source para APM para seus projetos Estude e&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Conheça algumas ferramentas open source para APM para seus projetos</p>
<hr>
<p>Estude e avalie essas boas ferramentas que você pode usar em seu projeto e infraestrutura.</p>
<h2 id="apache-skywalking">Apache Skywalking</h2>
<p>Site do projeto: <a href="https://skywalking.apache.org/">https://skywalking.apache.org/</a></p>
<p>GitHub do projeto: 14.7k stars e 250+ contributors</p>
<p>Recomendado para APPs rodando em Cloud e API.</p>
<p>Se você está imerso no universo de API, microserviços, kubernetes e cloud native esse é o projeto para você, ele foi desenhado para Cloud e API sendo hoje um dos maiores, mais respeitados e ativos do GitHub.</p>
<p>Há quem diga que é o melhor nesse segmento hoje.</p>
<h2 id="pinpoint">PinPoint</h2>
<p>Site do projeto: <a href="https://github.com/naver/pinpoint">https://github.com/naver/pinpoint</a></p>
<p>GitHub 10.8k stars e 90+ contributors</p>
<p>Acho que o PinPoint é o preferido da galera de JAVA em 2021, é bem conhecido e vastamente utilizado.</p>
<p>Recomendado para APPs JAVA ou Python</p>
<p>Baseado no Google’s Dapper.</p>
<h2 id="stagemonitor">StageMonitor</h2>
<p>Site do projeto <a href="https://www.stagemonitor.org">https://www.stagemonitor.org</a></p>
<p>GitHub 1.6k stars e 24+ contributors</p>
<p>Recomendado para JAVA, tem compatibilidade e integração com OpenTracing.</p>
<h2 id="scouter">Scouter</h2>
<p>Site do projeto <a href="https://github.com/scouter-project/scouter">https://github.com/scouter-project/scouter</a></p>
<p>GitHub 1.8k starts e 40+ contributors.</p>
<p>Inspirado no New Relic e APP Dynamics, é uma aplicação modular e bastante extensível.</p>
<p>Oferece interações nativas com Telegraf e Zipkin.</p>
<p>Tem agente nativo para java, linux, unix e até windows.</p>
<p>Monitora nativamente (usando telegraph)</p>
<ul>
<li>Redis</li>
<li>Nginx</li>
<li>Apache HTTPd</li>
<li>HAProxy</li>
<li>Kafka</li>
<li>MySQL</li>
<li>MongoDB</li>
<li>RabbitMQ</li>
<li>ElasticSearch</li>
<li>Kubernetes</li>
<li>Mesos</li>
</ul>
<p>Instrumenta nativamente:</p>
<ul>
<li>C#</li>
<li>GO</li>
<li>Java</li>
<li>JavaScript</li>
<li>Ruby</li>
<li>Scala</li>
<li>PHP</li>
</ul>
<p>Instrumentação extendida via código da comunidade</p>
<ul>
<li>C++</li>
<li>Python</li>
<li>Clojure</li>
<li>Elixir</li>
<li>Lua</li>
</ul>
<p>Veja dados completos da instrumentação</p>
<ul>
<li><a href="https://zipkin.io/pages/tracers_instrumentation.html">https://zipkin.io/pages/tracers_instrumentation.html</a> </li>
</ul>
<h2 id="app-metrics">APP Metrics</h2>
<p>Site do projeto <a href="https://www.app-metrics.io/">https://www.app-metrics.io/</a></p>
<p>GitHub 2k starts e 40+ contributors</p>
<p>Recomendado para APPs .Net.</p>
<p>Se o seu negócio é .NET o APP Metrics é a ferramenta pra você e seu time.</p>
<h2 id="inspectit-ocelot">InspectIT Ocelot</h2>
<p>Site do projeto: <a href="https://github.com/inspectIT/inspectit-ocelot">https://github.com/inspectIT/inspectit-ocelot</a></p>
<p>GitHub 158 starts e 25+ contributors</p>
<p>Esse é um projeto novo, o caçula da turma, mas tem muito potencial.</p>
<p>Ele tem uma pegada na fácil configuração, aliás zero configuração e uma boa oferta nativa de métricas.</p>
<p>E isso, bons estudos!</p>
<p>[s]</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>DROPS: Fazer Port Forward no K8S é fácil :)</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/k8s-port-forward-facil/"/>
        <id>https://gutocarvalho.net/k8s-port-forward-facil/</id>
            <category term="K8S"/>
            <category term="Drops"/>

        <updated>2021-10-22T09:09:30-03:00</updated>
            <summary>
                <![CDATA[
                    Aprenda a se conectar em um serviço K8S sem exposição! São DUMPs&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprenda a se conectar em um serviço K8S sem exposição!</p>
<hr>
<h3 id="o-que-são-drops">O que são drops?</h3>
<p>São DUMPs mentais rápidos e rasteiros, simples e objetivos – que funcionam. </p>
<p>Geralmente de algo que eu acabei de fazer.</p>
<p>Eu – quase sempre – volto para detalhar mais cada passo.</p>
<p>Considere com a mesma qualidade de um rascunho ou uma anotação rápida.</p>
<p>De qualquer forma comenta ai qquer coisa, os comentários estão ligados nos DROPS ;)</p>
<h3 id="demanda">Demanda!</h3>
<p>Preciso me conectar em um Mongo no cluster sem expor ele para o mundo.</p>
<h3 id="comofaz">ComoFaz?</h3>
<p>Vamos usar o port-forward, veja como a sintaxe é simples</p>
<pre><code>kubectl nome_do_pod porta_local:portal_cluster -n seuname_space
</code></pre>
<p>Exemplo!</p>
<pre><code>kubectl mongodb-replicaset-0 28015:27017 -n seuname_space
</code></pre>
<p>Prontinho!</p>
<h3 id="refs">Refs</h3>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/</a></li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>DROPS: Baixando imagens de ECR Privado no K8S</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/baixando-imagens-de-ecr-privado-no-k8s/"/>
        <id>https://gutocarvalho.net/baixando-imagens-de-ecr-privado-no-k8s/</id>
            <category term="K8S"/>
            <category term="Drops"/>

        <updated>2021-10-15T05:55:59-03:00</updated>
            <summary>
                <![CDATA[
                    Aprenda a pegar imagens de registry ECR privado em seu cluster K8S.
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprenda a pegar imagens de registry ECR privado em seu cluster K8S.</p>
<hr>
<h3 id="o-que-são-drops">O que são drops?</h3>
<p>São DUMPs mentais rápidos e rasteiros, simples e objetivos – que funcionam. </p>
<p>Geralmente de algo que eu acabei de fazer.</p>
<p>Eu – quase sempre – volto para detalhar mais cada passo.</p>
<p>Considere com a mesma qualidade de um rascunho ou uma anotação rápida.</p>
<p>De qualquer forma comenta ai qquer coisa, os comentários estão ligados nos DROPS ;)</p>
<h3 id="demanda">Demanda!</h3>
<p>Precisamos cadastrar um secret para pegar imagens de um ECR privado</p>
<h3 id="comofaz">ComoFaz?</h3>
<p>Primeiro vamos gerar o TOKEN, você precisa do AWS CLI para isso,</p>
<pre><code>aws configure
</code></pre>
<p>depois de configurado gere o TOKEN</p>
<pre><code>aws ecr get-login-password --region us-east-1
</code></pre>
<p>agora vamos no K8S criar o secret</p>
<pre><code>kubectl create secret docker-registry ecr-nativetrail \
  --docker-server=ID.dkr.ecr.us-east-1.amazonaws.com/ \
  --docker-username=AWS \
  --docker-password=SEU_TOKEN_GERADO_VIA_AWS_CLI \
  --namespace=SEU_NAMESPACE
</code></pre>
<p>verifique</p>
<pre><code>kubectl get secrets -n SEU_NAMESPACE|grep ecr
</code></pre>
<p>saída</p>
<pre><code>ecr-nativetrail kubernetes.io/dockerconfigjson        1      15h
</code></pre>
<p>Nao se esqueça de setar o secret no seu manifesto de deployment, dentro de “spec” </p>
<pre><code>      spec:
        imagePullSecrets:
        - name: ecr-nativetrail
</code></pre>
<p>Exemplo</p>
<pre><code>  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: app-example
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: app-example
    template:
      metadata:
        labels:
          app: nativetrail
      spec:
        imagePullSecrets:
        - name: ecr-nativetrail
        containers:
        - name: app-examp;e
          image:  ID.dkr.ecr.us-east-1.amazonaws.com/app:latest
</code></pre>
<p>Prontinho!</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>DROPS: Storage EBS em seu Cluster K8S EC2</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/storageclass-ebs-em-seu-cluster-k8s-ec2/"/>
        <id>https://gutocarvalho.net/storageclass-ebs-em-seu-cluster-k8s-ec2/</id>
            <category term="K8S"/>
            <category term="Drops"/>

        <updated>2021-10-20T16:49:46-03:00</updated>
            <summary>
                <![CDATA[
                    Aprenda a usar o storage EBS como SC em seu cluster EC2!&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprenda a usar o storage EBS como SC em seu cluster EC2!</p>
<hr>
<h3 id="o-que-são-drops">O que são drops?</h3>
<p>São DUMPs mentais rápidos e rasteiros, simples e objetivos – que funcionam. </p>
<p>Geralmente de algo que eu acabei de fazer.</p>
<p>Eu – quase sempre – volto para detalhar mais cada passo.</p>
<p>Considere com a mesma qualidade de um rascunho ou uma anotação rápida.</p>
<p>De qualquer forma comenta ai qquer coisa, os comentários estão ligados nos DROPS ;)</p>
<h3 id="demanda">Demanda</h3>
<p>Configurar um storageclass EBS em um cluster K8S rodando via EC2.</p>
<h3 id="comofaz">ComoFaz?</h3>
<p>crie o arquivo aws-ebs-secrets.yaml</p>
<pre><code>vim ws-ebs-secrets.yaml
</code></pre>
<p>insira o conteúdo abaixo, e ajuste suas credenciais</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: aws-secret
  namespace: kube-system
stringData:
  key_id: &quot;SUA_ACCESS_KEY&quot;
  access_key: &quot;SUA_SECRET_KEY&quot;
</code></pre>
<p>aplique o manifesto</p>
<pre><code>kubectl apply -f aws-ebs-secrets.yaml
</code></pre>
<p>agora instale o driver ebs</p>
<pre><code>kubectl apply -k &quot;github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.4&quot;
</code></pre>
<p>saída esperada</p>
<pre><code>serviceaccount/ebs-csi-controller-sa created
serviceaccount/ebs-csi-node-sa created
clusterrole.rbac.authorization.k8s.io/ebs-csi-node-role created
clusterrole.rbac.authorization.k8s.io/ebs-external-attacher-role created
clusterrole.rbac.authorization.k8s.io/ebs-external-provisioner-role created
clusterrole.rbac.authorization.k8s.io/ebs-external-resizer-role created
clusterrole.rbac.authorization.k8s.io/ebs-external-snapshotter-role created
clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-attacher-binding created
clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-node-getter-binding created
clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-provisioner-binding created
clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-resizer-binding created
clusterrolebinding.rbac.authorization.k8s.io/ebs-csi-snapshotter-binding created
deployment.apps/ebs-csi-controller created
poddisruptionbudget.policy/ebs-csi-controller created
daemonset.apps/ebs-csi-node created
csidriver.storage.k8s.io/ebs.csi.aws.com created
</code></pre>
<p>verifique se tá tudo bem</p>
<pre><code>kubectl get pods -n kube-system|grep ebs
</code></pre>
<p>saída esperada</p>
<pre><code>ebs-csi-controller-7c486f7676-826ph        6/6     Running     0          110s
ebs-csi-controller-7c486f7676-l5wsg        6/6     Running     0          110s
ebs-csi-node-5q7lk                         3/3     Running     0          108s
ebs-csi-node-cxv4p                         3/3     Running     0          109s
ebs-csi-node-fskfw                         3/3     Running     0          108s
</code></pre>
<p>tá tudo bem, agora crie o manifesto do storageclass</p>
<pre><code>vim ebs-storageclass.yaml
</code></pre>
<p>insira o conteúdo abaixo</p>
<pre><code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ebs-storageclass
parameters:
  type: gp2
provisioner: ebs.csi.aws.com
volumeBindingMode: Immediate
</code></pre>
<p>aplique o manifesto</p>
<pre><code>kubectl apply -f ebs-storageclass.yaml
</code></pre>
<p>verifique se criou o sc</p>
<pre><code>kubectl get sc
</code></pre>
<p>saída esperada</p>
<pre><code>NAME                         PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
ebs-storageclass             ebs.csi.aws.com   Delete          Immediate   false                  3m24s
</code></pre>
<p>caso queira, defina este sc como default</p>
<pre><code>  kubectl patch storageclass ebs-storageclass -p &#39;{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}
</code></pre>
<p>prontinho! :)</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>DROPS: Criando um eks básico via terraform</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/drops-criando-um-eks-basico-via-terraform/"/>
        <id>https://gutocarvalho.net/drops-criando-um-eks-basico-via-terraform/</id>
            <category term="Drops"/>

        <updated>2021-10-15T05:56:08-03:00</updated>
            <summary>
                <![CDATA[
                    Aprenda a criar um cluster EKS básico via Terraform! Só vem :”)&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprenda a criar um cluster EKS básico via Terraform! Só vem :”)</p>
<h3 id="o-que-são-drops">O que são drops?</h3>
<p>São DUMPs mentais rápidos e rasteiros, simples e objetivos – que funcionam. </p>
<p>Geralmente eu volto para detalhar mais cada passo – com o devido tempo.</p>
<p>Considere com a mesma qualidade de um rascunho ou uma anotação rápida.</p>
<p>De qualquer forma comenta ai qquer coisa, os comentários estão ligados nos DROPS ;)</p>
<hr>
<h3 id="requisitos">requisitos</h3>
<p>Tenha kubeclt e helm instalados.</p>
<p>Tenha o awscli e aws-iam-authenticator instalados.</p>
<p>Esteja logado com aws-cli na conta correta com permissoes para criar o EKS.</p>
<p>Quais permissões?</p>
<ul>
<li><a href="https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/iam-permissions.md">https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/iam-permissions.md</a></li>
</ul>
<h3 id="crie-um-diretório-para-seu-projeto">crie um diretório para seu projeto</h3>
<pre><code>mkdir -p terraform/eks/meucluster
cd terraform/eks/meucluster
</code></pre>
<h3 id="crie-o-arquivo-para-definir-as-versões-dos-módulos">crie o arquivo para definir as versões dos módulos</h3>
<p>edite o arquiovo</p>
<pre><code>vim defaults.tf
</code></pre>
<p>insira o conteúdo abaixo para definir der forma clara as versões que vamos usar</p>
<pre><code>terraform {
  required_version = &quot;&gt;= 0.13.1&quot;

  required_providers {
    aws        = &quot;&gt;= 3.62.0&quot;
    local      = &quot;&gt;= 2.1.0&quot;
    kubernetes = &quot;&gt;= 2.5.0&quot;
    cloudinit  = &quot;&gt;= 2.2.0&quot;
    http = {
      source  = &quot;terraform-aws-modules/http&quot;
      version = &quot;&gt;= 2.4.1&quot;
    }
  }
}

provider &quot;aws&quot; {
  region  = &quot;us-east-1&quot;
}

data &quot;aws_caller_identity&quot; &quot;current&quot; {} # used for accesing Account ID and ARN
</code></pre>
<p>valide</p>
<pre><code>terraform validate
</code></pre>
<p>saída esperada</p>
<pre><code>Success! The configuration is valid.
</code></pre>
<h3 id="crie-o-arquivo-com-as-variáveis-que-vão-configurar-a-vpc-e-o-eks">crie o arquivo com as variáveis que vão configurar a vpc e o eks</h3>
<p>crie o arquivo</p>
<pre><code>vim configs.tfvars
</code></pre>
<p>coloque o conteúdo</p>
<pre><code>## configuracoes para vpc

cluster_name            = &quot;hackathon&quot;
name_prefix             = &quot;hackathon&quot;
main_network_block      = &quot;10.0.0.0/16&quot;
subnet_prefix_extension = 4
zone_offset             = 8

## configuracoes para eks

eks_cluster_version = &quot;1.21&quot;

node_group_default_disk_size = 30
node_group_default_ami_type = &quot;AL2_x86_64&quot;
    
node_group_desired_capacity = 1
node_group_max_capacity = 8
node_group_min_capacity = 1
node_group_max_unavailable_percentage = 50
    
node_group_capacity_type = &quot;ON_DEMAND&quot;
node_group_instance_type = &quot;t3.large&quot;
</code></pre>
<p>valide</p>
<pre><code>terraform validate
</code></pre>
<p>saída esperada</p>
<pre><code>Success! The configuration is valid.
</code></pre>
<h3 id="crie-o-manifesto-que-vai-criar-e-configurar-a-vpc">crie o manifesto que vai criar e configurar a VPC</h3>
<p>vamos criar agora a vpc</p>
<pre><code>vim vpc.tl
</code></pre>
<p>insira o conteúdo abaixo</p>
<pre><code># Declarando variaveis

variable &quot;cluster_name&quot; {
  type        = string
  description = &quot;Nome do Cluster&quot;
}

variable &quot;name_prefix&quot; {
  type        = string
  description = &quot;Prefixo que será utilizado nos nomes dos objetos de infraestrutura criados na AWS&quot;
}

variable &quot;main_network_block&quot; {
  type        = string
  description = &quot;Bloco CIDR base que sera usado em sua VPC&quot;
}

variable &quot;subnet_prefix_extension&quot; {
  type        = number
  description = &quot;Bits de extensao do seu bloco CIDR que sera usado para calcular as suberedes&quot;
}

variable &quot;zone_offset&quot; {
  type        = number
  description = &quot;Offset de extensao dos bits CIDR para calculo e subnets, para evitar colisoes entre redes publicas e privadas&quot;
}

# Fixando o IP do Nat Gateway

resource &quot;aws_eip&quot; &quot;nat_gw_elastic_ip&quot; {
  vpc = true
  tags = {
    Name            = &quot;${var.cluster_name}-nat-eip&quot;
  }
}

# Criando a VPC usando configs do modulo oficial AWS

module &quot;vpc&quot; {
  source  = &quot;terraform-aws-modules/vpc/aws&quot;

  name = &quot;${var.name_prefix}-vpc&quot;
  cidr = var.main_network_block
  azs  = [ &quot;us-east-1a&quot;, &quot;us-east-1b&quot;, &quot;us-east-1c&quot;, &quot;us-east-1d&quot; ]

  private_subnets      = [&quot;10.0.1.0/24&quot;, &quot;10.0.2.0/24&quot;, &quot;10.0.3.0/24&quot;, &quot;10.0.4.0/24&quot; ]
  public_subnets       = [&quot;10.0.5.0/24&quot;, &quot;10.0.6.0/24&quot;, &quot;10.0.7.0/24&quot;, &quot;10.0.8.0/24&quot; ]

  enable_nat_gateway     = true
  single_nat_gateway     = true
  one_nat_gateway_per_az = false
  enable_dns_hostnames   = true
  reuse_nat_ips          = true
  external_nat_ip_ids    = [aws_eip.nat_gw_elastic_ip.id]

  # Adicionando tags necessarias para o EKS

  tags = {
    &quot;kubernetes.io/cluster/${var.cluster_name}&quot; = &quot;shared&quot;
  }
  public_subnet_tags = {
    &quot;kubernetes.io/cluster/${var.cluster_name}&quot; = &quot;shared&quot;
    &quot;kubernetes.io/role/elb&quot;                    = &quot;1&quot;
  }
  private_subnet_tags = {
    &quot;kubernetes.io/cluster/${var.cluster_name}&quot; = &quot;shared&quot;
    &quot;kubernetes.io/role/internal-elb&quot;           = &quot;1&quot;
  }
}
</code></pre>
<p>valide</p>
<pre><code>terraform validate
</code></pre>
<p>saída esperada</p>
<pre><code>Success! The configuration is valid.
</code></pre>
<h3 id="crie-o-manifesto-que-vai-configurar-o-eks">crie o manifesto que vai configurar o eks</h3>
<p>crie o arquivo</p>
<pre><code>vim eks.tf
</code></pre>
<p>coloque o conteúdo</p>
<pre><code># declarando variáveis

variable &quot;eks_cluster_version&quot; {
  type        = string
}

variable &quot;node_group_default_disk_size&quot; {
  type        = number
}

variable &quot;node_group_default_ami_type&quot; {
  type        = string
}

variable &quot;node_group_desired_capacity&quot; {
  type        = number
}

variable &quot;node_group_max_capacity&quot; {
  type        = number
}

variable &quot;node_group_min_capacity&quot; {
  type        = number
}

variable &quot;node_group_max_unavailable_percentage&quot; {
  type        = number
}

variable &quot;node_group_capacity_type&quot; {
  type        = string
}

variable &quot;node_group_instance_type&quot; {
  type        = string
}


# dados para configuracao

data &quot;aws_eks_cluster&quot; &quot;eks&quot; {
  name = module.eks.cluster_id
}

data &quot;aws_eks_cluster_auth&quot; &quot;eks&quot; {
  name = module.eks.cluster_id
}

# configurando provider kubernetes

provider &quot;kubernetes&quot; {
  host                   = data.aws_eks_cluster.eks.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.eks.certificate_authority[0].data)
  token                  = data.aws_eks_cluster_auth.eks.token
}

# configurando eks

module &quot;eks&quot; {
  source          = &quot;terraform-aws-modules/eks/aws&quot;

  cluster_version = var.eks_cluster_version
  cluster_name    = &quot;${var.cluster_name}&quot;

  subnets = module.vpc.private_subnets
  vpc_id  = module.vpc.vpc_id

 node_groups_defaults = {
    ami_type  = var.node_group_default_ami_type
    disk_size = var.node_group_default_disk_size
  }

  node_groups = {
    hacka_node_group = {
      desired_capacity = var.node_group_desired_capacity
      max_capacity     = var.node_group_max_capacity
      min_capacity     = var.node_group_min_capacity

      instance_types = [ var.node_group_instance_type ]
      capacity_type  = var.node_group_capacity_type
      update_config = {
        max_unavailable_percentage = var.node_group_max_unavailable_percentage
      }
    }
  }

}
</code></pre>
<p>valide</p>
<pre><code>terraform validate
</code></pre>
<p>saída esperada</p>
<pre><code>Success! The configuration is valid.
</code></pre>
<h3 id="crie-o-plano-de-instalação">crie o plano de instalação</h3>
<p>Use o arquivo configs.tfvars para construir o plano</p>
<pre><code>terraform plan -out=plan -var-file=configs.tfvars;
</code></pre>
<p>saída esperada</p>
<pre><code>.
.
.

Plan: 54 to add, 0 to change, 0 to destroy.
</code></pre>
<h3 id="aplique-o-plano-criado">aplique o plano criado</h3>
<p>vamos aplicar agora</p>
<pre><code>terraform apply plan
</code></pre>
<p>saída esperada</p>
<pre><code>.
.
.
Apply complete! Resources: 54 added, 0 changed, 0 destroyed.
</code></pre>
<h3 id="validando-o-acesso-ao-cluster">validando o acesso ao cluster</h3>
<p>carregue o kubconfig</p>
<pre><code>export KUBECONFIG=kubeconfig
kubectl cluster-info
</code></pre>
<p>saída esperada</p>
<pre><code>Kubernetes control plane is running at https://xxxx.gr7.us-east-1.eks.amazonaws.com
CoreDNS is running at https://xxx.gr7.us-east-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
</code></pre>
<p>verifique os nodes</p>
<pre><code>kubectl get nodes
</code></pre>
<p>saída esperada</p>
<pre><code>NAME                        STATUS   ROLES    AGE     VERSION
ip-10-xx-xx-xx.ec2.internal   Ready    &lt;none&gt;   3m11s   v1.21.2-eks-55daa9d
</code></pre>
<p>Parece tudo ok!</p>
<p>Agora vai na AWS e confere VPC, EKS e o restante ;)    </p>
<h3 id="post-install">post-install</h3>
<p>Agora precisamos instalar o ingress e o cert-manager, vai lá!</p>
<ol>
<li><a href="https://gutocarvalho.net/instalando-ingress-aws-no-k8s-eks/">https://gutocarvalho.net/instalando-ingress-aws-no-k8s-eks/</a></li>
<li><a href="https://gutocarvalho.net/instalando-cert-manager-no-k8s/">https://gutocarvalho.net/instalando-cert-manager-no-k8s/</a></li>
</ol>
<p>Caso precise regerar o kubeconfig</p>
<ul>
<li><a href="https://gutocarvalho.net/gerando-kubeconfig-para-aws-eks/">https://gutocarvalho.net/gerando-kubeconfig-para-aws-eks/</a></li>
</ul>
<p>Bons estudos!</p>
<h3 id="refs">refs</h3>
<ul>
<li><a href="https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest">https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest</a></li>
<li><a href="https://github.com/terraform-aws-modules/terraform-aws-eks">https://github.com/terraform-aws-modules/terraform-aws-eks</a></li>
<li><a href="https://itnext.io/build-an-eks-cluster-with-terraform-d35db8005963">https://itnext.io/build-an-eks-cluster-with-terraform-d35db8005963</a></li>
<li><a href="https://adamtheautomator.com/terraform-eks-module/">https://adamtheautomator.com/terraform-eks-module/</a></li>
<li><a href="https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/iam-permissions.md">https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/iam-permissions.md</a></li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>DROPS: Instalando GitLab Runner Dind no Ubuntu</title>
        <author>
            <name>Guto Carvalho</name>
        </author>
        <link href="https://gutocarvalho.net/instalando-gitlab-runner-no-ubuntu/"/>
        <id>https://gutocarvalho.net/instalando-gitlab-runner-no-ubuntu/</id>
            <category term="Drops"/>

        <updated>2021-10-15T05:56:12-03:00</updated>
            <summary>
                <![CDATA[
                    <p>Aprenda a instalar o gitlab-runner para rodar docker DIND.</p>

                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Aprenda a instalar o gitlab-runner para rodar docker DIND.</p>

<p><strong>O que são drops?<br><br></strong>São DUMPs mentais rápidos e rasteiros, simples e objetivos – que funcionam. <br><br>Geralmente eu volto para detalhar mais cada passo – com o devido tempo.<br><br>Considere com a mesma qualidade de um rascunho ou uma anotação rápida.<br><br>De qualquer forma comenta ai qquer coisa, os comentários estão ligados nos DROPS ;)</p>
<hr>
<p><strong>Instalando docker no Ubuntu</strong><br><br><code>curl https://releases.rancher.com/install-docker/19.03.sh | sh</code><br><code>systemctl enable docker</code><br><code>systemctl start docker</code><br><br><strong>Instalando o GitLab Runner no Ubuntu</strong><br><br><code>curl -LJO "https://gitlab-runner-downloads.s3.amazonaws.com/latest/deb/gitlab-runner_amd64.deb"</code><br><code>dpkg -i gitlab-runner_amd64.deb</code><br><code>systemctl enable gitlab-runner</code><br><code>systemctl start gitlab-runner</code><br><br><strong>Registrando um Runner Docker-in-Docker</strong><br><br><code>gitlab-runner register -n \</code><br><code>  --url https://gitlab.com/ \</code><br><code>  --registration-token TOKEN \</code><br><code>  --description "Runner Guto 1" \</code><br><code>  --tag-list docker \</code><br><code>  --executor docker \</code><br><code>  --docker-image "docker:19.03.12" \</code><br><code>  --docker-privileged true \</code><br><code>  --docker-volumes /var/run/docker.sock:/var/run/docker.sock </code><br><strong><br></strong>Crie quantos runners achar necessário, eu costumo reservar 1 vCPU e 1 GB de RAM para cada Runner.<br><br>Um exemplo, se eu subir 4 runners, a VM vai ter 4 vCPUs + 4 GB de RAM só para os Runners, normalmente deixo mais 1 vCPU + 1 GB pro OS e assim fica tranquilo para cenários bem básicos.</p>
<p><strong>Configurando a concorrência do gitlab-runner</strong><br><br>edite o arquivo<br><br><code>vim  /etc/gitlab-runner/config.toml</code><br><br>altere o número de runners concorrentes para a quantidade que você criou.<br><br><code>concurrent = 4</code><br><br>reinicie o runner</p>
<p><code>systemctl restart gitlab-runner</code></p>
<p>ah, tenha certeza que a instância tem recursos que aguentam a demanda de concorrência, tanto do ponto de vista de memória, quanto de processamento e disco.</p>
<p>[s]<br>Guto</p>
            ]]>
        </content>
    </entry>
</feed>
